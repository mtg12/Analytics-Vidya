# A project to analyse tweets to identify hate speech
#https://towardsdatascience.com/another-twitter-sentiment-analysis-bb5b01ebad90

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
cols=['sentiment','id','date','query_string','user','text']
df=pd.read_csv("train_data.csv",header=None,names=cols,encoding = "ISO-8859-1")

#print (df.head())
#print(df.sentiment.value_counts())

train=df.drop(['id','date','query_string','user'],axis=1)
train['pre_clean_length']=[len(t) for t in train.text]
#print(train.head())

fig,ax=plt.subplots(figsize=(5,5))
plt.boxplot(train.pre_clean_length)
#plt.show()

# Data Cleaning

# HTML Decoding
from bs4 import BeautifulSoup
ex1=BeautifulSoup(df.text[279],"html5lib")

# Using regular expressiona to clean @
import re
#print(train.text[343])
#print(re.sub(r'@[A-Za-z0-9]+','',df.text[343]))

# Clean URL Links
#print(df.text[0])
#print(re.sub('https?://[A-Za-z0-9./]+','',df.text[0]))

#UTF-8 BOM Byte Order Mark
print(df.text[226])

# remove Hashtag/ numbers
print(df.text[175])
print(re.sub("[^a-zA-Z]", " ", df.text[175]))

from nltk.tokenize import WordPunctTokenizer
tok=WordPunctTokenizer()
pat1 = r'@[A-Za-z0-9]+'
pat2 = r'https?://[A-Za-z0-9./]+'
combined_pat = r'|'.join((pat1, pat2))

def tweet_cleaner(text):
    soup = BeautifulSoup(text, 'lxml')
    souped = soup.get_text()
    stripped = re.sub(combined_pat, '', souped)
    try:
        clean = stripped.decode("utf-8-sig").replace(u"\ufffd", "?")
    except:
        clean = stripped
    letters_only = re.sub("[^a-zA-Z]", " ", clean)
    lower_case = letters_only.lower()
    # During the letters_only process two lines above, it has created unnecessay white spaces,
    # I will tokenize and join together to remove unneccessary white spaces
    words = tok.tokenize(lower_case)
    return (" ".join(words)).strip()

#testing = train.text[:100]
#test_result = []
#for t in testing:
#    test_result.append(tweet_cleaner(t))

#print(test_result)

nums = [0,400000,800000,1200000,1600000]
print("Cleaning and parsing the tweets...\n")
clean_tweet_texts = []
for i in range(nums[0],nums[1]):
    if( (i+1)%10000 == 0 ):
        print("Tweets %d of %d has been processed" % ( i+1, nums[1] ))
    clean_tweet_texts.append(tweet_cleaner(df['text'][i]))

clean_df = pd.DataFrame(clean_tweet_texts,columns=['text'])
clean_df['target'] = df.sentiment
print(clean_df.head())

clean_df.to_csv('clean_tweet.csv',encoding='utf-8')
csv = 'clean_tweet.csv'

my_df = pd.read_csv(csv,index_col=0)
print(my_df.head())
